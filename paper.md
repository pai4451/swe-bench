Absolutely. Here's a more **expert-level, detailed slide deck** that integrates the **figures and arguments from the original paper** to present *Passerine: Agentic Program Repair in an Industrial Codebase* convincingly to your boss.

This version focuses on:

* Incorporating concrete experimental results (e.g. Fig 3, 4, 7, 8, 9)
* Citing detailed comparisons (SWE-Bench vs GITS, Human vs Machine bugs)
* Framing insights as **strategic takeaways for internal engineering or AI adoption**

---

## ğŸ§  Detailed Slide Deck â€” Expert-Level Review of "Passerine"

---

### âœ… Slide 1: Title Slide

**Title:**
ğŸ“˜ *Passerine: Industrial-Scale Agentic Program Repair at Google*
**Subtitle:**
Insights from Deploying LLM Agents on Real Bugs at Scale

---

### âœ… Slide 2: Motivation

**Title:**
Why This Paper Matters
**Content:**

* Existing APR (Automated Program Repair) focuses on academic, small-scale bugs
* Google deploys LLM-based agent (Passerine) in their internal codebase
* Evaluates it on 178 curated bugs (from 80K+ real issues in GITS)
* First large-scale analysis of agentic repair on multilingual, production-grade monorepos

---

### âœ… Slide 3: System Architecture

**Title:**
Passerine Agent Design (Fig. 5)
**Content:**

* ReAct-style loop: *"Think â†’ Act â†’ Observe"*
* 5 core commands: `code_search`, `cat`, `edit`, `bazel`, `finish`
* Operates entirely via API-bound commands â€” no full Linux shell needed
* Accesses internal infra: Code Search, Test infra, Filesystem containers
* No hardcoded flow (no state machine)

ğŸ“Š Use Fig. 5 to show the modular, minimal design

---

### âœ… Slide 4: Benchmark Construction

**Title:**
GITS-Eval: A Real-World, Curated Benchmark (Fig. 1)
**Content:**

* Constructed via 4-phase funnel (manual + automatic filtering)
* Final benchmark = 178 bugs:

  * 78 Human-reported
  * 50 SAN (Sanitizer errors)
  * 50 TOD (Test Order Dependency)
* Compared against SWE-Bench:

  * Larger, multilingual patches
  * More dispersed edits
  * Fewer identifiable code tokens in descriptions

ğŸ“Š Show Fig. 1 (pipeline), support with Fig. 3(aâ€“e)

---

### âœ… Slide 5: Dataset Characteristics

**Title:**
GITS vs SWE-Bench: Real-World Bugs Are Harder (Fig. 3)
**Content:**

* GITS bugs have:

  * Fewer code-identifiable tokens â†’ harder to search (Fig. 3a)
  * Patches span more files (3b)
  * Higher patch spread (3c)
  * More hunk edits (3d)
  * More total lines changed (3e)
* Human bugs especially sparse in signals

ğŸ“Š Use Fig. 3 side-by-side with bullet explanations

---

### âœ… Slide 6: Evaluation Setup

**Title:**
Evaluation Methodology
**Content:**

* Gemini 1.5 Pro (no fine-tuning)
* 20 independent trajectories per bug
* Max 25 steps each
* Patch types:

  * **Plausible**: passes ground-truth test
  * **Valid**: semantically equivalent to ground-truth
* All manual annotation by 3 reviewers
* Separate evaluation for machine vs human bugs

---

### âœ… Slide 7: Key Result 1 â€“ Patch Success

**Title:**
Patch Generation Success (Fig. 7)
**Content:**

* Passerine generates at least 1 plausible patch in:

  * 78% SAN bugs
  * 68% TOD bugs
  * 25.6% Human bugs
* Valid patch rates (manually annotated):

  * 62% SAN
  * 24% TOD
  * 17.9% Human

ğŸ“Š Show Fig. 7(a)(b) â€” plausibility vs validity divergence
ğŸ§  Insight: Machine bugs benefit from structured reproduction; human bugs suffer from sparse info

---

### âœ… Slide 8: Key Result 2 â€“ Bug Report Structure Matters

**Title:**
Machine Bugs Yield Better Localization (Fig. 9)
**Content:**

* Even in failure cases (no plausible patch):

  * 54% of machine-bug trajectories still edited correct file
  * Only 3.5% for human-bug trajectories
* File-System Distance used as metric (Fig. 9)
  ğŸ§  Insight:
  Structured bug reports (repro steps, test links) directly enable agent alignment

---

### âœ… Slide 9: Key Result 3 â€“ Adaptive Strategy by Bug Type (Fig. 8)

**Title:**
Agent Self-Adapts to Bug Structure
**Content:**

* Human bugs â†’ starts with `code_search`, `cat`
* Machine bugs â†’ starts with `bazel`, later switches to `edit`
* Fig. 8 shows command usage over 25 steps per bug type
  ğŸ§  Insight: No strategy engine needed â€” prompt alone allows strategic adaptation

---

### âœ… Slide 10: Trajectory Smells for Optimization

**Title:**
Behavioral Smells Reveal Failure Modes (Table V)
**Content:**

* Failures often show:

  * No test execution (`NO_TEST_SMELL`)
  * Repeated reads/edits (`NO_OP_CAT`, `CONSECUTIVE_EDIT`)
  * Excessive searching (`CONSECUTIVE_SEARCH`)
* Table V shows smell incidence vs bug type & success
  ğŸ§  Insight: These metrics can guide agent pruning & early aborts

---

### âœ… Slide 11: Design & Future Plans

**Title:**
Design Decisions & Research Opportunities
**Content:**

* Limitations:

  * No test synthesis (future: agent-generated tests)
  * Context overflow (2M tokens max)
  * Lacks patch diversity (e.g., beam search)
* Improvements planned:

  * Sliding windows or memory summarization
  * Abstention predictor (avoid expensive no-op runs)
  * Expansion of supported tools (e.g., link/file access)

---

### âœ… Slide 12: Strategic Takeaways

**Title:**
What This Tells Us
**Content:**
âœ… Lightweight agents with minimal tool API can fix real bugs
âœ… Agent success highly sensitive to bug report structure
âœ… Multi-agent architectures + prompt engineering = next frontier
âœ… Googleâ€™s monorepo strategy, code search infra, and standardized test runners made this viable
âœ… Internal evaluation of LLM-based repair agents is feasible â€” starting with machine-generated bugs

---

Would you like me to export this to a PowerPoint/Keynote format or generate speaker notes next?



 Summary: Industrial-Scale Agentic Program Repair at Google
ğŸ“Š Benchmark â€“ GITS-Eval (178 Real Bugs)
Curated from Googleâ€™s internal issue tracking system (GITS), the GITS-Eval dataset includes:

78 human-reported bugs

100 machine-reported bugs, split into:

50 Sanitizer bugs (via static analysis)

50 TOD bugs (Test Order Dependency)

All bugs are reproducible, testable, and aligned with agent capabilities.

ğŸ¤– Agent â€“ Passerine
A lightweight LLM-based agent for automated repair, designed to operate within Googleâ€™s developer ecosystem.

Architecture: ReAct loop (Think â†’ Act â†’ Observe)

Command set:

cat: View file contents

code_search: Search monorepo

edit: Modify source

bazel: Build & run tests

finish: Terminate repair

Minimalist yet expressive â€” fully API-integrated into Googleâ€™s infra

ğŸ“ˆ Results (Using Gemini 1.5 Pro, 20 trajectories/bug)
Bug Type	Plausible Patch Rate	Valid Patch Rate (Manual)
ğŸ§  Human Bugs	25.6%	17.9%
âš™ï¸ Machine Bugs	73.0%	43.0%

ğŸŸ¢ Machine-reported bugs are significantly easier to fix due to structured bug descriptions (e.g., test targets, repro info).
ğŸ”µ Human-reported bugs show challenges in localization and patch precision.

ğŸ§  Key Insight:
Bug report quality, not just model capability, is a primary driver of agent performance.


ğŸ§ª Why GITS-Eval Matters: A Realistic Benchmark for Enterprise-Scale APR
1. ğŸ“Š Google-Built Benchmark: GITS-Eval (178 Bugs)
A curated evaluation set sourced from Googleâ€™s internal issue tracking system (GITS), covering:

78 Human-reported bugs

100 Machine-reported bugs, split into:

50 from automated sanitizers (e.g., null dereference, buffer overflows)

50 from test-order dependency analyzers (e.g., test A influences test B)

ğŸ§  Carefully filtered to ensure agent suitability: reproducible, testable, patchable.

2. ğŸ” GITS-Eval â‰  SWE-Bench: Fundamental Differences
By comparing 2,000 GITS bugs to SWE-Bench, the authors highlight major divergences:

Language diversity: SWE-Bench is Python-only; GITS includes C++, Java, Go, etc.

Patch complexity: GITS fixes span more files, more lines, higher dispersion.

Bug description signal: GITS often lacks code-like terms, increasing localization difficulty.

ğŸ§  A model that excels on SWE-Bench may underperform in real-world industrial settings.

3. ğŸš« SWE-Bench â‰  Enterprise Reality
The authors explicitly do not claim Passerine performs well on SWE-Bench or SWE-Bench-Lite.

ğŸ§  Their stance: Enterprise bugs require enterprise-grade benchmarks.
SWE-Bench is not representative of the scale, diversity, or structure of production bugs faced in practice.

âœ… Key Takeaway
â€œTo evaluate real-world agentic repair, we must move beyond open-source benchmarks like SWE-Bench.
GITS-Eval was built to reflect the real challenges engineers face at scale â€” and models must be tested accordingly.â€

Building GITS-Eval: A Rigorous, Multi-Phase Curation Pipeline
ğŸ§¾ Goal:
To construct a high-quality, representative, and tractable bug benchmark for evaluating agentic repair (Passerine) in a realistic enterprise setting.

ğŸ—ï¸ Step-by-Step Filtering Process (Based on Fig. 1)
Phase	Purpose	Key Filters
Phase 0	Initial filtering of 80K+ bugs from GITS (2024/6/21 â€“ 8/30)	âœ… Verified bugs only
âœ… Single patch per bug
âœ… Non-empty description
âœ… Internal-only
â›” Removed ~60K unsuitable bugs (non-reproducible, no test/source change)	
â¡ï¸ Remaining: 15K human bugs + 1K machine bugs (SAN/TOD)	
Phase 1	Ensure changes are meaningful and testable	âœ… Patch affects code/test files
âœ… Plausibility is measurable
Phase 2	Ensure feasibility for agent evaluation	â›” Remove bugs with:
ğŸ“¸ multimedia content
ğŸ“„ overly large patches
â±ï¸ long test times
â¡ï¸ Remaining: 7K human, 1K machine bugs	
Phase 3	Final curation for high precision & representativeness	âœ… Manual filtering to remove:
ğŸ”¹ flaky tests
ğŸ”¹ magic constants
ğŸ”¹ test inadequacies
âœ… Final sets: 78 curated human bugs, 100 curated machine bugs	

âœ… Final Outcome: GITS-Eval = 178 Bugs
Source Type	Count
Human-reported bugs	78
Machine-reported bugs (SAN + TOD)	100

ğŸ§  GITS-Eval balances realism and feasibility â€” making it a credible foundation for evaluating LLM-based agents in industrial codebases.
